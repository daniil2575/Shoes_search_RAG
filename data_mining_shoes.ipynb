{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7df2b40",
   "metadata": {},
   "source": [
    "# data_mining (shoes)\n",
    "\n",
    "Полный ноутбук для сбора/очистки/нормализации каталога обуви и загрузки в таблицу bikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7eb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 1) Config & imports ====\n",
    "import os, re, time, math, json, random, itertools, typing as T\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# DB\n",
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "\n",
    "# Paths & env\n",
    "USE_GPU = os.getenv(\"USE_GPU\", \"false\").lower() == \"true\"\n",
    "DB_DSN  = os.getenv(\"DB_DSN\", \"postgresql://postgres:postgres@localhost:5430/bikes\")\n",
    "OUT_JSON = os.getenv(\"OUT_JSON\", \"./data/parsed_data.json\")\n",
    "OUT_PARQUET = os.getenv(\"OUT_PARQUET\", \"./data/parsed_data.parquet\")\n",
    "\n",
    "# polite scraping\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/125.0 Safari/537.36\"\n",
    "}\n",
    "REQ_TIMEOUT = 30\n",
    "SLEEP_RANGE = (0.5, 1.8)   # seconds between requests\n",
    "\n",
    "print(\"DB_DSN:\", DB_DSN)\n",
    "print(\"USE_GPU:\", USE_GPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6bf7c",
   "metadata": {},
   "source": [
    "## Источники и адаптеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8118a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 2) Sources and adapters ====\n",
    "SOURCES = [\n",
    "    # Примеры — замените на реальные:\n",
    "    {\n",
    "        \"name\": \"shop_example_men_sneakers\",\n",
    "        \"list_url\": \"https://example.com/men/sneakers\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"shop_example_women_boots\",\n",
    "        \"list_url\": \"https://example.com/women/boots\",\n",
    "    },\n",
    "]\n",
    "\n",
    "def fetch(url: str) -> str:\n",
    "    \"\"\"HTTP GET с повторами и паузами.\"\"\"\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT)\n",
    "            if r.status_code == 200:\n",
    "                return r.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(1.5 + i)\n",
    "    return \"\"\n",
    "\n",
    "def extract_links_example(html: str) -> list:\n",
    "    \"\"\"Черновой пример извлечения ссылок со страницы списка (замените под реальный сайт).\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = []\n",
    "    for card in soup.select(\".product-card a\"):\n",
    "        href = card.get(\"href\")\n",
    "        if href and href.startswith(\"/\"):\n",
    "            href = \"https://example.com\" + href\n",
    "        if href and href.startswith(\"http\"):\n",
    "            links.append(href)\n",
    "    # dedup\n",
    "    return list(dict.fromkeys(links))\n",
    "\n",
    "def parse_product_example(url: str) -> dict:\n",
    "    \"\"\"Черновой разбор карточки товара для example.com. Замените селекторы под реальный сайт.\"\"\"\n",
    "    html = fetch(url)\n",
    "    if not html:\n",
    "        return {}\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def txt(sel, default=\"\"):\n",
    "        el = soup.select_one(sel)\n",
    "        return el.get_text(strip=True) if el else default\n",
    "\n",
    "    name = txt(\"h1.product-title\")\n",
    "    brand = txt(\".product-brand\")\n",
    "    category = txt(\".breadcrumb .active\") or \"sneakers\"\n",
    "    price_raw = txt(\".price-current\").replace(\" \",\" \").replace(\"₽\",\"\").replace(\",\",\".\")\n",
    "    price = None\n",
    "    try:\n",
    "        price = float(__import__(\"re\").findall(r\"[0-9.]+\", price_raw)[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    color = txt(\".color-name\")\n",
    "    gender = txt(\".gender-tag\")  # может отсутствовать\n",
    "    sizes_text = \" \".join(x.get_text(strip=True) for x in soup.select(\".sizes .size\"))\n",
    "    desc = txt(\".product-description\")\n",
    "\n",
    "    image_url = \"\"\n",
    "    im = soup.select_one(\".product-images img\")\n",
    "    if im and (im.get(\"src\") or im.get(\"data-src\")):\n",
    "        image_url = im.get(\"data-src\") or im.get(\"src\")\n",
    "        if image_url.startswith(\"/\"):\n",
    "            image_url = \"https://example.com\" + image_url\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"brand\": brand,\n",
    "        \"category\": category,\n",
    "        \"gender\": gender,\n",
    "        \"color\": color,\n",
    "        \"sizes_raw\": sizes_text,\n",
    "        \"price\": price,\n",
    "        \"old_price\": None,\n",
    "        \"discount\": None,\n",
    "        \"in_stock\": None,\n",
    "        \"url\": url,\n",
    "        \"image_url\": image_url,\n",
    "        \"description\": desc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc144480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 3) Normalization helpers ====\n",
    "def norm_gender(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    if \"жен\" in s or \"women\" in s: return \"women\"\n",
    "    if \"муж\" in s or \"men\" in s:   return \"men\"\n",
    "    if \"дет\" in s or \"kid\" in s:   return \"kids\"\n",
    "    return \"\"\n",
    "\n",
    "def parse_sizes(text: str) -> list[int]:\n",
    "    import re\n",
    "    nums = []\n",
    "    for part in re.findall(r\"\\b\\d{2}(?:-\\d{2})?\\b\", text or \"\"):\n",
    "        if \"-\" in part:\n",
    "            a,b = map(int, part.split(\"-\"))\n",
    "            nums.extend(range(min(a,b), max(a,b)+1))\n",
    "        else:\n",
    "            try:\n",
    "                nums.append(int(part))\n",
    "            except:\n",
    "                pass\n",
    "    return [x for x in nums if 16 <= x <= 52]\n",
    "\n",
    "def norm_color(s: str) -> str:\n",
    "    import re\n",
    "    s = (s or \"\").lower()\n",
    "    if re.search(r\"черн|black|noir|schwarz\", s): return \"black\"\n",
    "    if re.search(r\"бел|white|blanc|weiß\", s): return \"white\"\n",
    "    if re.search(r\"син|blue|bleu\", s): return \"blue\"\n",
    "    if re.search(r\"красн|red|rouge\", s): return \"red\"\n",
    "    if re.search(r\"коричн|brown|brun\", s): return \"brown\"\n",
    "    if re.search(r\"беж|beige\", s): return \"beige\"\n",
    "    if re.search(r\"сер|grey|gray|grau\", s): return \"grey\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_season(text: str) -> str:\n",
    "    t = (text or \"\").lower()\n",
    "    return \"winter\" if \"зим\" in t else \"summer\" if \"лет\" in t else \"demi\" if \"демисез\" in t else \"\"\n",
    "\n",
    "def extract_materials(text: str) -> dict:\n",
    "    t = (text or \"\").lower()\n",
    "    return {\n",
    "        \"mat_leather\": int(\"кож\" in t or \"leather\" in t),\n",
    "        \"mat_suede\": int(\"замш\" in t or \"suede\" in t),\n",
    "        \"mat_textile\": int(\"текстил\" in t or \"textile\" in t),\n",
    "        \"membrane\": int(\"gore\" in t or \"мембран\" in t),\n",
    "    }\n",
    "\n",
    "def build_full_description(rec: dict) -> str:\n",
    "    sizes = rec.get(\"sizes\", []) or []\n",
    "    sizes_str = f\"размеры: {', '.join(map(str, sorted(set(sizes))))}\" if sizes else \"\"\n",
    "    parts = [\n",
    "        rec.get(\"name\",\"\"),\n",
    "        rec.get(\"brand\",\"\"),\n",
    "        rec.get(\"category\",\"\"),\n",
    "        rec.get(\"gender\",\"\"),\n",
    "        rec.get(\"color\",\"\"),\n",
    "        sizes_str,\n",
    "        rec.get(\"description\",\"\"),\n",
    "    ]\n",
    "    # синонимы по категории\n",
    "    cat = (rec.get(\"category\",\"\") + \" \" + rec.get(\"name\",\"\")).lower()\n",
    "    synonyms = []\n",
    "    if \"sneaker\" in cat or \"крос\" in cat: synonyms += [\"кроссовки\", \"sneakers\"]\n",
    "    if \"boot\" in cat or \"ботин\" in cat:   synonyms += [\"ботинки\", \"boots\"]\n",
    "    if \"sandal\" in cat or \"сандал\" in cat: synonyms += [\"сандалии\", \"sandals\"]\n",
    "    if \"loaf\" in cat or \"лофер\" in cat:   synonyms += [\"лоферы\", \"loafers\"]\n",
    "    if synonyms: parts.append(\" \".join(synonyms))\n",
    "    return \" | \".join([p for p in parts if p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 4) Crawl orchestration ====\n",
    "def crawl_source(src: dict) -> list[dict]:\n",
    "    print(\">>> crawl:\", src[\"name\"], src[\"list_url\"])\n",
    "    html = fetch(src[\"list_url\"])\n",
    "    if not html:\n",
    "        print(\"  ! no html\")\n",
    "        return []\n",
    "    links = extract_links_example(html)  # замените на адаптер для вашего сайта\n",
    "    print(\"  found links:\", len(links))\n",
    "\n",
    "    items = []\n",
    "    for i, url in enumerate(links, 1):\n",
    "        it = parse_product_example(url)   # замените на адаптер для вашего сайта\n",
    "        if it:\n",
    "            items.append(it)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  parsed {i}/{len(links)}\")\n",
    "        time.sleep(random.uniform(*SLEEP_RANGE))\n",
    "    return items\n",
    "\n",
    "all_raw = []\n",
    "for src in SOURCES:\n",
    "    try:\n",
    "        all_raw += crawl_source(src)\n",
    "    except Exception as e:\n",
    "        print(\"source failed:\", src[\"name\"], e)\n",
    "\n",
    "print(\"Total raw items:\", len(all_raw))\n",
    "df = pd.DataFrame(all_raw)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f921522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 5) Normalize dataframe ====\n",
    "if df.empty:\n",
    "    print(\"WARNING: df is empty — заполни SOURCES и адаптеры под реальный сайт.\")\n",
    "else:\n",
    "    df[\"gender\"] = df[\"gender\"].map(norm_gender)\n",
    "    df[\"color_norm\"] = df[\"color\"].map(norm_color)\n",
    "    df[\"sizes\"] = df[\"sizes_raw\"].map(parse_sizes)\n",
    "    df[\"season\"] = df[\"description\"].map(extract_season)\n",
    "    mats = df[\"description\"].map(extract_materials).apply(pd.Series).fillna(0).astype(int)\n",
    "    df = pd.concat([df, mats], axis=1)\n",
    "\n",
    "    # full_description\n",
    "    df[\"full_description\"] = df.apply(lambda r: build_full_description({\n",
    "        \"name\": r.get(\"name\",\"\"),\n",
    "        \"brand\": r.get(\"brand\",\"\"),\n",
    "        \"category\": r.get(\"category\",\"\"),\n",
    "        \"gender\": r.get(\"gender\",\"\"),\n",
    "        \"color\": r.get(\"color_norm\",\"\"),\n",
    "        \"sizes\": r.get(\"sizes\",[]),\n",
    "        \"description\": r.get(\"description\",\"\")\n",
    "    }), axis=1)\n",
    "\n",
    "    # базовые проверки\n",
    "    df = df.dropna(subset=[\"name\", \"url\"]).drop_duplicates(subset=[\"url\"])\n",
    "    print(\"Prepared rows:\", len(df))\n",
    "    display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 6) Embeddings ====\n",
    "if df.empty:\n",
    "    print(\"Skip embeddings: df empty\")\n",
    "else:\n",
    "    device = \"cuda\" if USE_GPU else \"cpu\"\n",
    "    embedder = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=device)\n",
    "    vecs = []\n",
    "    for t in df[\"full_description\"].fillna(\"\").tolist():\n",
    "        v = embedder.encode([t], prompt_name=\"query\")[0]\n",
    "        vecs.append(v)\n",
    "    df[\"embedding\"] = vecs\n",
    "    print(\"Embeddings computed:\", len(vecs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 7) DB load ====\n",
    "if df.empty:\n",
    "    print(\"Skip DB load: df empty\")\n",
    "else:\n",
    "    with psycopg.connect(DB_DSN) as conn:\n",
    "        register_vector(conn)\n",
    "        with conn.cursor() as cur:\n",
    "            # опционально: добавить UNIQUE(url) для аккуратного UPSERT\n",
    "            # cur.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS ux_bikes_url ON bikes(url);\")\n",
    "            # conn.commit()\n",
    "\n",
    "            rows = df.to_dict(orient=\"records\")\n",
    "            for r in rows:\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO bikes (name, brand, category, price, old_price, discount, in_stock,\n",
    "                                       url, image_url, full_description, embedding, fts_vector)\n",
    "                    VALUES (%(name)s, %(brand)s, %(category)s, %(price)s, %(old_price)s, %(discount)s, %(in_stock)s,\n",
    "                            %(url)s, %(image_url)s, %(full_description)s, %(embedding)s,\n",
    "                            to_tsvector('russian', coalesce(%(full_description)s,'')))\n",
    "                    ON CONFLICT (url) DO UPDATE SET\n",
    "                        name=EXCLUDED.name,\n",
    "                        brand=EXCLUDED.brand,\n",
    "                        category=EXCLUDED.category,\n",
    "                        price=EXCLUDED.price,\n",
    "                        old_price=EXCLUDED.old_price,\n",
    "                        discount=EXCLUDED.discount,\n",
    "                        in_stock=EXCLUDED.in_stock,\n",
    "                        image_url=EXCLUDED.image_url,\n",
    "                        full_description=EXCLUDED.full_description,\n",
    "                        embedding=EXCLUDED.embedding,\n",
    "                        fts_vector=to_tsvector('russian', coalesce(EXCLUDED.full_description,''))\n",
    "                    \"\"\",\n",
    "                    r\n",
    "                )\n",
    "        conn.commit()\n",
    "    print(\"DB load complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce7c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 8) Save artifacts ====\n",
    "if not df.empty:\n",
    "    os.makedirs(os.path.dirname(OUT_JSON), exist_ok=True)\n",
    "    df.to_json(OUT_JSON, orient=\"records\", force_ascii=False, indent=2)\n",
    "    print(\"Saved JSON ->\", OUT_JSON)\n",
    "\n",
    "    try:\n",
    "        df.to_parquet(OUT_PARQUET, index=False)\n",
    "        print(\"Saved Parquet ->\", OUT_PARQUET)\n",
    "    except Exception as e:\n",
    "        print(\"Parquet save skipped:\", e)\n",
    "else:\n",
    "    print(\"Skip save: df empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c362756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 9) Optional: quick preview ====\n",
    "test_query = \"мужские кроссовки черные 42\"\n",
    "with psycopg.connect(DB_DSN) as conn:\n",
    "    register_vector(conn)\n",
    "    with conn.cursor() as cur:\n",
    "        # FTS\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT name, price, url\n",
    "            FROM bikes\n",
    "            WHERE fts_vector @@ websearch_to_tsquery('russian', %s)\n",
    "            ORDER BY ts_rank(fts_vector, websearch_to_tsquery('russian', %s)) DESC\n",
    "            LIMIT 5\n",
    "            \"\"\",\n",
    "            (test_query, test_query)\n",
    "        )\n",
    "        print(\"FTS top-5:\")\n",
    "        for r in cur.fetchall():\n",
    "            print(\" -\", r)\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
